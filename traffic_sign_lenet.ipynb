{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import keras\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten\n",
    "from keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras import optimizers\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import glob\n",
    "from skimage import io, transform, exposure, color\n",
    "\n",
    "def preprocess(img):\n",
    "    img = transform.resize(img,(28,28))\n",
    "    hsv = color.rgb2hsv(img)\n",
    "    hsv[:,:,2] = exposure.equalize_hist(hsv[:,:,2])\n",
    "    return color.hsv2rgb(hsv)\n",
    "\n",
    "def read_img(path):\n",
    "    images = []\n",
    "    for img_path in glob.glob(path+\"/*.ppm\"):\n",
    "        img = io.imread(img_path)\n",
    "        img = preprocess(img)\n",
    "        images.append(img)\n",
    "    return images\n",
    "\n",
    "def load_data(path):\n",
    "    img_list = list()\n",
    "    labels = list()\n",
    "    for f in listdir(path):\n",
    "        images = read_img(path+'/'+f)\n",
    "        mul = len(images)\n",
    "        img_list = img_list+images\n",
    "        labels = labels + [int(f)]*mul\n",
    "    return np.array(img_list,dtype = 'float32'),np.array(labels,dtype = 'float32')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/skimage/transform/_warps.py:84: UserWarning: The default mode, 'constant', will be changed to 'reflect' in skimage 0.15.\n",
      "  warn(\"The default mode, 'constant', will be changed to 'reflect' in \"\n"
     ]
    }
   ],
   "source": [
    "train_path = \"/media/nishanth/E/bigdatathings/ML-DL/traffic-signs/GTSRB/Final_Training/train\"\n",
    "train_x, train_y =  load_data(train_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "train_y = to_categorical(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data (27804, 28, 28, 3)\n",
      "training labels (27804, 43)\n",
      "validation data (6952, 28, 28, 3)\n",
      "validation labels (6952, 43)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_val, y_train, y_val = train_test_split(train_x,train_y,test_size=0.2,random_state=42)\n",
    "print \"training data\",x_train.shape\n",
    "print \"training labels\",y_train.shape\n",
    "print \"validation data\",x_val.shape\n",
    "print \"validation labels\",y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class :  2\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f5661bb48d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = x_val[200]\n",
    "plt.imshow(img)\n",
    "print \"class : \",np.argmax(y_val[200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                     input_shape=(28, 28, 3),\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "   \n",
    "    \n",
    "    \n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "   \n",
    "\n",
    "    model.add(Conv2D(128, (3, 3), padding='same',\n",
    "                     activation='relu'))\n",
    "    model.add(Conv2D(128, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "    \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(43, activation='softmax'))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "          optimizer=sgd,\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, min_lr=1e-5)\n",
    "checkpoint = ModelCheckpoint('traffic_sign_model.h5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 27804 samples, validate on 6952 samples\n",
      "Epoch 1/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.7170 - acc: 0.8070Epoch 00001: val_acc improved from -inf to 0.90521, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 170s 6ms/step - loss: 0.7165 - acc: 0.8072 - val_loss: 0.3059 - val_acc: 0.9052\n",
      "Epoch 2/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.1185 - acc: 0.9649Epoch 00002: val_acc improved from 0.90521 to 0.95900, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 187s 7ms/step - loss: 0.1185 - acc: 0.9649 - val_loss: 0.1335 - val_acc: 0.9590\n",
      "Epoch 3/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0608 - acc: 0.9825Epoch 00003: val_acc did not improve\n",
      "27804/27804 [==============================] - 194s 7ms/step - loss: 0.0607 - acc: 0.9825 - val_loss: 0.1395 - val_acc: 0.9563\n",
      "Epoch 4/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0435 - acc: 0.9869Epoch 00004: val_acc improved from 0.95900 to 0.98763, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 197s 7ms/step - loss: 0.0435 - acc: 0.9869 - val_loss: 0.0443 - val_acc: 0.9876\n",
      "Epoch 5/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0332 - acc: 0.9900Epoch 00005: val_acc did not improve\n",
      "27804/27804 [==============================] - 195s 7ms/step - loss: 0.0333 - acc: 0.9900 - val_loss: 0.0535 - val_acc: 0.9842\n",
      "Epoch 6/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0226 - acc: 0.9929Epoch 00006: val_acc did not improve\n",
      "27804/27804 [==============================] - 200s 7ms/step - loss: 0.0226 - acc: 0.9929 - val_loss: 0.0689 - val_acc: 0.9794\n",
      "Epoch 7/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0189 - acc: 0.9946Epoch 00007: val_acc did not improve\n",
      "27804/27804 [==============================] - 202s 7ms/step - loss: 0.0189 - acc: 0.9946 - val_loss: 0.0653 - val_acc: 0.9810\n",
      "Epoch 8/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0077 - acc: 0.9979Epoch 00008: val_acc improved from 0.98763 to 0.99381, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 200s 7ms/step - loss: 0.0077 - acc: 0.9979 - val_loss: 0.0251 - val_acc: 0.9938\n",
      "Epoch 9/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0040 - acc: 0.9991- ETA: 6Epoch 00009: val_acc improved from 0.99381 to 0.99482, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 203s 7ms/step - loss: 0.0040 - acc: 0.9991 - val_loss: 0.0236 - val_acc: 0.9948\n",
      "Epoch 10/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0034 - acc: 0.9992Epoch 00010: val_acc improved from 0.99482 to 0.99525, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 203s 7ms/step - loss: 0.0034 - acc: 0.9992 - val_loss: 0.0219 - val_acc: 0.9953\n",
      "Epoch 11/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0029 - acc: 0.9995Epoch 00011: val_acc did not improve\n",
      "27804/27804 [==============================] - 201s 7ms/step - loss: 0.0029 - acc: 0.9995 - val_loss: 0.0227 - val_acc: 0.9947\n",
      "Epoch 12/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0030 - acc: 0.9992Epoch 00012: val_acc did not improve\n",
      "27804/27804 [==============================] - 200s 7ms/step - loss: 0.0030 - acc: 0.9992 - val_loss: 0.0223 - val_acc: 0.9945\n",
      "Epoch 13/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9997Epoch 00013: val_acc improved from 0.99525 to 0.99540, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 204s 7ms/step - loss: 0.0021 - acc: 0.9997 - val_loss: 0.0208 - val_acc: 0.9954\n",
      "Epoch 14/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9998Epoch 00014: val_acc improved from 0.99540 to 0.99554, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 202s 7ms/step - loss: 0.0020 - acc: 0.9998 - val_loss: 0.0207 - val_acc: 0.9955\n",
      "Epoch 15/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 1.0000- ETA: 3s - loss: 0.0Epoch 00015: val_acc did not improve\n",
      "27804/27804 [==============================] - 198s 7ms/step - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0199 - val_acc: 0.9953\n",
      "Epoch 16/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0019 - acc: 0.9997Epoch 00016: val_acc did not improve\n",
      "27804/27804 [==============================] - 201s 7ms/step - loss: 0.0019 - acc: 0.9997 - val_loss: 0.0210 - val_acc: 0.9954\n",
      "Epoch 17/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0018 - acc: 0.9997Epoch 00017: val_acc did not improve\n",
      "27804/27804 [==============================] - 201s 7ms/step - loss: 0.0018 - acc: 0.9997 - val_loss: 0.0189 - val_acc: 0.9955\n",
      "Epoch 18/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9998- ETA: 3s - loss: 0.00Epoch 00018: val_acc did not improve\n",
      "27804/27804 [==============================] - 201s 7ms/step - loss: 0.0015 - acc: 0.9998 - val_loss: 0.0201 - val_acc: 0.9951\n",
      "Epoch 19/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0017 - acc: 0.9997Epoch 00019: val_acc did not improve\n",
      "27804/27804 [==============================] - 207s 7ms/step - loss: 0.0017 - acc: 0.9997 - val_loss: 0.0197 - val_acc: 0.9954\n",
      "Epoch 20/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0016 - acc: 0.9998Epoch 00020: val_acc did not improve\n",
      "27804/27804 [==============================] - 210s 8ms/step - loss: 0.0016 - acc: 0.9998 - val_loss: 0.0205 - val_acc: 0.9947\n",
      "Epoch 21/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0014 - acc: 0.9998Epoch 00021: val_acc did not improve\n",
      "27804/27804 [==============================] - 209s 8ms/step - loss: 0.0014 - acc: 0.9998 - val_loss: 0.0202 - val_acc: 0.9955\n",
      "Epoch 22/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0015 - acc: 0.9996Epoch 00022: val_acc did not improve\n",
      "27804/27804 [==============================] - 203s 7ms/step - loss: 0.0015 - acc: 0.9996 - val_loss: 0.0198 - val_acc: 0.9955\n",
      "Epoch 23/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0011 - acc: 0.9999Epoch 00023: val_acc did not improve\n",
      "27804/27804 [==============================] - 214s 8ms/step - loss: 0.0011 - acc: 0.9999 - val_loss: 0.0200 - val_acc: 0.9955\n",
      "Epoch 24/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0012 - acc: 0.9999Epoch 00024: val_acc improved from 0.99554 to 0.99568, saving model to traffic_sign_model.h5\n",
      "27804/27804 [==============================] - 238s 9ms/step - loss: 0.0013 - acc: 0.9999 - val_loss: 0.0198 - val_acc: 0.9957\n",
      "Epoch 25/25\n",
      "27776/27804 [============================>.] - ETA: 0s - loss: 0.0013 - acc: 0.9998Epoch 00025: val_acc did not improve\n",
      "27804/27804 [==============================] - 243s 9ms/step - loss: 0.0013 - acc: 0.9998 - val_loss: 0.0200 - val_acc: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1bcc03850>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=25, validation_data=(x_val,y_val), callbacks=[reduce_lr, checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_data = pd.read_csv(\"/media/nishanth/E/bigdatathings/ML-DL/traffic-signs/GTSRB/Final_Test/GT-final_test.csv\",sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_test = np.empty((12630,28,28,3),dtype=np.float32)\n",
    "y_test = []\n",
    "path = \"/media/nishanth/E/bigdatathings/ML-DL/traffic-signs/GTSRB/Final_Test/Images\"\n",
    "for idx,image_path in enumerate(glob.glob(path+\"/*.ppm\")):\n",
    "    img = io.imread(image_path)\n",
    "    img = preprocess(img)\n",
    "    label = int(test_data.loc[test_data['Filename']==image_path[-9:]]['ClassId'])\n",
    "    x_test[idx] = img\n",
    "    y_test.append(label)\n",
    "\n",
    "y_test = to_categorical(np.array(y_test,dtype = 'float32'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Data Augmentation improves accuracy\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False, \n",
    "                            featurewise_std_normalization=False, \n",
    "                            width_shift_range=0.1,\n",
    "                            height_shift_range=0.1,\n",
    "                            zoom_range=0.2,\n",
    "                            shear_range=0.1,\n",
    "                            rotation_range=10.,)\n",
    "\n",
    "datagen.fit(x_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "869/869 [==============================] - 182s 210ms/step - loss: 1.4952 - acc: 0.5647 - val_loss: 0.5371 - val_acc: 0.8284\n",
      "Epoch 2/25\n",
      "869/869 [==============================] - 183s 210ms/step - loss: 0.3835 - acc: 0.8785 - val_loss: 0.1103 - val_acc: 0.9679\n",
      "Epoch 3/25\n",
      "869/869 [==============================] - 183s 210ms/step - loss: 0.2283 - acc: 0.9270 - val_loss: 0.1206 - val_acc: 0.9619\n",
      "Epoch 4/25\n",
      "869/869 [==============================] - 186s 214ms/step - loss: 0.1768 - acc: 0.9428 - val_loss: 0.0668 - val_acc: 0.9796\n",
      "Epoch 5/25\n",
      "869/869 [==============================] - 195s 224ms/step - loss: 0.1361 - acc: 0.9565 - val_loss: 0.0633 - val_acc: 0.9796\n",
      "Epoch 6/25\n",
      "869/869 [==============================] - 189s 218ms/step - loss: 0.1159 - acc: 0.9619 - val_loss: 0.0432 - val_acc: 0.9866\n",
      "Epoch 7/25\n",
      "869/869 [==============================] - 211s 243ms/step - loss: 0.1020 - acc: 0.9679 - val_loss: 0.0378 - val_acc: 0.9869\n",
      "Epoch 8/25\n",
      "869/869 [==============================] - 196s 226ms/step - loss: 0.0913 - acc: 0.9704 - val_loss: 0.0345 - val_acc: 0.9895\n",
      "Epoch 9/25\n",
      "869/869 [==============================] - 189s 218ms/step - loss: 0.0833 - acc: 0.9731 - val_loss: 0.0263 - val_acc: 0.9925\n",
      "Epoch 10/25\n",
      "869/869 [==============================] - 207s 238ms/step - loss: 0.0776 - acc: 0.9744 - val_loss: 0.0331 - val_acc: 0.9905\n",
      "Epoch 11/25\n",
      "869/869 [==============================] - 222s 255ms/step - loss: 0.0696 - acc: 0.9776 - val_loss: 0.0165 - val_acc: 0.9945\n",
      "Epoch 12/25\n",
      "869/869 [==============================] - 200s 230ms/step - loss: 0.0653 - acc: 0.9792 - val_loss: 0.0194 - val_acc: 0.9934\n",
      "Epoch 13/25\n",
      "869/869 [==============================] - 205s 236ms/step - loss: 0.0639 - acc: 0.9800 - val_loss: 0.0222 - val_acc: 0.9934\n",
      "Epoch 14/25\n",
      "869/869 [==============================] - 205s 236ms/step - loss: 0.0666 - acc: 0.9797 - val_loss: 0.0131 - val_acc: 0.9958\n",
      "Epoch 15/25\n",
      "869/869 [==============================] - 205s 236ms/step - loss: 0.0519 - acc: 0.9837 - val_loss: 0.0129 - val_acc: 0.9964\n",
      "Epoch 16/25\n",
      "869/869 [==============================] - 206s 237ms/step - loss: 0.0557 - acc: 0.9825 - val_loss: 0.0192 - val_acc: 0.9944\n",
      "Epoch 17/25\n",
      "869/869 [==============================] - 206s 237ms/step - loss: 0.0506 - acc: 0.9838 - val_loss: 0.0181 - val_acc: 0.9941\n",
      "Epoch 18/25\n",
      "869/869 [==============================] - 207s 238ms/step - loss: 0.0494 - acc: 0.9846 - val_loss: 0.0135 - val_acc: 0.9961\n",
      "Epoch 19/25\n",
      "869/869 [==============================] - 203s 234ms/step - loss: 0.0321 - acc: 0.9899 - val_loss: 0.0057 - val_acc: 0.9984\n",
      "Epoch 20/25\n",
      "869/869 [==============================] - 208s 239ms/step - loss: 0.0303 - acc: 0.9903 - val_loss: 0.0066 - val_acc: 0.9976\n",
      "Epoch 21/25\n",
      "869/869 [==============================] - 209s 241ms/step - loss: 0.0254 - acc: 0.9925 - val_loss: 0.0062 - val_acc: 0.9983\n",
      "Epoch 22/25\n",
      "869/869 [==============================] - 210s 242ms/step - loss: 0.0249 - acc: 0.9919 - val_loss: 0.0067 - val_acc: 0.9974\n",
      "Epoch 23/25\n",
      "869/869 [==============================] - 210s 242ms/step - loss: 0.0198 - acc: 0.9933 - val_loss: 0.0051 - val_acc: 0.9986\n",
      "Epoch 24/25\n",
      "869/869 [==============================] - 211s 243ms/step - loss: 0.0206 - acc: 0.9941 - val_loss: 0.0052 - val_acc: 0.9986\n",
      "Epoch 25/25\n",
      "869/869 [==============================] - 208s 239ms/step - loss: 0.0178 - acc: 0.9947 - val_loss: 0.0051 - val_acc: 0.9984\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f56618cb550>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit_generator(datagen.flow(x_train, y_train, batch_size=32), epochs=25, validation_data=(x_val,y_val), callbacks=[reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12630/12630 [==============================] - 28s 2ms/step\n",
      "test_set accuracy : 98.4323040352\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test,y_test)\n",
    "print \"test_set accuracy :\",score[1]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
